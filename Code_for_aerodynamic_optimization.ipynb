{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunshine-sutingjun/RL-optimization-for-airfoils/blob/main/Code_for_aerodynamic_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuTSrpKQrmht"
      },
      "source": [
        "Structure of DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToH7JKcsrVas"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the DNN structure as per the hyperparameters provided\n",
        "def build_dnn(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='selu', input_shape=(input_shape,)),\n",
        "        Dropout(0.1),\n",
        "        Dense(128, activation='selu'),\n",
        "        Dropout(0.1),\n",
        "        Dense(128, activation='selu'),\n",
        "        Dropout(0.1),\n",
        "        Dense(64, activation='selu'),\n",
        "        Dropout(0.1),\n",
        "        Dense(1)  # The output layer has one neuron to predict the lift-drag ratio\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Assuming the CST array has 14 parameters as input\n",
        "input_shape = 14  # Number of CST parameters\n",
        "\n",
        "# Build the DNN model\n",
        "dnn_model = build_dnn(input_shape)\n",
        "\n",
        "# Compile the model with the Adam optimizer and mean squared error loss function\n",
        "dnn_model.compile(optimizer=Adam(learning_rate=0.001),  # Start with a learning rate of 0.001\n",
        "                  loss='mean_squared_error')\n",
        "\n",
        "# Model Summary\n",
        "dnn_model.summary()\n",
        "\n",
        "# You would also need to set up the training procedure\n",
        "# Define the number of training iterations and the minibatch size\n",
        "training_iterations = 6000  # Training iterations\n",
        "batch_size = 64  # Minibatch size\n",
        "\n",
        "# The actual training code would involve fitting the model on your dataset\n",
        "# dnn_model.fit(x_train, y_train, epochs=training_iterations, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Ni-EAw0lob"
      },
      "source": [
        "DDQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rh-g9lAq0oMP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import collections\n",
        "import random\n",
        "\n",
        "# Load your airfoil dataset\n",
        "def load_airfoil_dataset(file_path):\n",
        "    return np.load(file_path)\n",
        "\n",
        "# Define the bounds for the CST parameters\n",
        "def define_bounds():\n",
        "    return (np.array([...]), np.array([...]))  # Fill in with actual bounds\n",
        "\n",
        "# Deep Neural Network for CL/CD prediction\n",
        "def build_dnn(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "        Dropout(0.1),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.1),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.1),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.1),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def compile_dnn(dnn_model):\n",
        "    dnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Environment and simulation\n",
        "def environment(state, action, dnn_model, state_lower_bound, state_upper_bound):\n",
        "    next_state = np.clip(state + action, state_lower_bound, state_upper_bound)\n",
        "    current_cl_cd = dnn_model.predict(state.reshape(1, -1)).flatten()\n",
        "    next_cl_cd = dnn_model.predict(next_state.reshape(1, -1)).flatten()\n",
        "    reward = 2.5 * (next_cl_cd - current_cl_cd)\n",
        "    e = int(np.any(next_state < state_lower_bound) or np.any(next_state > state_upper_bound))\n",
        "    return next_state, reward, e\n",
        "\n",
        "# Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        minibatch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, es = zip(*minibatch)\n",
        "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(es)\n",
        "\n",
        "# Q-Network\n",
        "def create_q_network(num_states, num_actions, hidden_sizes):\n",
        "    inputs = tf.keras.Input(shape=(num_states,))\n",
        "    x = inputs\n",
        "    for size in hidden_sizes:\n",
        "        x = Dense(size, activation='relu')(x)\n",
        "    outputs = Dense(num_actions)(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Training process\n",
        "def train_ddqn(airfoil_dataset, state_lower_bound, state_upper_bound, dnn_model):\n",
        "    # Define training parameters\n",
        "    num_states = airfoil_dataset.shape[1]\n",
        "    num_actions = 28  # Assuming this is known\n",
        "    hidden_sizes = [128] * 4\n",
        "    buffer_capacity = 10000\n",
        "    batch_size = 64\n",
        "    episodes = 1000  # Total episodes to train\n",
        "    epsilon = 1.0\n",
        "    epsilon_min = 0.01\n",
        "    epsilon_decay = 0.995\n",
        "    gamma = 0.95  # Discount factor\n",
        "\n",
        "    # Create Q-Network and Target Network\n",
        "    q_network = create_q_network(num_states, num_actions, hidden_sizes)\n",
        "    target_network = create_q_network(num_states, num_actions, hidden_sizes)\n",
        "    target_network.set_weights(q_network.get_weights())\n",
        "    \n",
        "    # Optimizer for the Q-network\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    \n",
        "    # Replay buffer for experience replay\n",
        "    replay_buffer = ReplayBuffer(buffer_capacity)\n",
        "    \n",
        "    # Training loop\n",
        "    for episode in range(episodes):\n",
        "        state = airfoil_dataset[np.random.choice(len(airfoil_dataset))]\n",
        "        total_reward = 0\n",
        "\n",
        "        for t in range(T):  # Maximum number of steps per episode, T should be defined\n",
        "            if np.random.rand() < epsilon:\n",
        "                # Assuming actions are somehow bounded, action selection mechanism is needed\n",
        "                action = np.random.uniform(low=-1, high=1, size=(num_actions,))  # Random action\n",
        "            else:\n",
        "                action_values = q_network.predict(state.reshape(1, -1))\n",
        "                action = np.argmax(action_values[0])\n",
        "\n",
        "            next_state, reward, e = environment(state, action, dnn_model, state_lower_bound, state_upper_bound)\n",
        "            total_reward += reward\n",
        "            replay_buffer.add((state, action, reward, next_state, e))\n",
        "            \n",
        "            # Experience replay\n",
        "            if len(replay_buffer.buffer) >= batch_size:\n",
        "                states, actions, rewards, next_states, es = replay_buffer.sample(batch_size)\n",
        "                target_q_values = target_network.predict(next_states)\n",
        "                targets = rewards + gamma * np.max(target_q_values, axis=1) * (1 - es)\n",
        "                \n",
        "                with tf.GradientTape() as tape:\n",
        "                    q_values = q_network(states)\n",
        "                    q_values = tf.reduce_sum(q_values * tf.one_hot(actions, num_actions), axis=1)\n",
        "                    loss = tf.keras.losses.mean_squared_error(targets, q_values)\n",
        "                \n",
        "                gradients = tape.gradient(loss, q_network.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
        "\n",
        "            if e == 1:\n",
        "                break  # Episode ends if design requirements are not met\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        # Update epsilon\n",
        "        if epsilon > epsilon_min:\n",
        "            epsilon *= epsilon_decay\n",
        "        \n",
        "        # Update target network weights\n",
        "        if episode % target_update_frequency == 0:  # target_update_frequency to be defined\n",
        "            target_network.set_weights(q_network.get_weights())\n",
        "        \n",
        "        print(f\"Episode {episode}: Total reward: {total_reward}\")\n",
        "    \n",
        "    # Save the final trained Q-network model\n",
        "    q_network.save('final_q_network.h5')\n",
        "\n",
        "# Main entry point\n",
        "def main():\n",
        "    file_path = 'path_to_your_airfoil_dataset.npy'  # Replace with your dataset's actual path\n",
        "    airfoil_dataset = load_airfoil_dataset(file_path)\n",
        "    state_lower_bound, state_upper_bound = define_bounds()\n",
        "    \n",
        "    # Create and compile the DNN for CL/CD prediction\n",
        "    dnn_model = build_dnn(14)  # Assuming there are 14 CST parameters\n",
        "    compile_dnn(dnn_model)\n",
        "    \n",
        "    # Load trained weights if available\n",
        "    # dnn_model.load_weights('path_to_trained_dnn_weights.h5')\n",
        "    \n",
        "    # Train the DDQN with the loaded dataset and DNN model\n",
        "    train_ddqn(airfoil_dataset, state_lower_bound, state_upper_bound, dnn_model)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO9N8Kn2Lh4AuThyPSzkjbQ",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

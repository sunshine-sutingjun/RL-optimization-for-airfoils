{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunshine-sutingjun/RL-optimization-for-airfoils/blob/main/Code_for_aerodynamic_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuTSrpKQrmht"
      },
      "source": [
        "Structure of DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToH7JKcsrVas"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the DNN structure as per the hyperparameters provided\n",
        "def build_dnn(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='selu', input_shape=(input_shape,)),\n",
        "        Dropout(0.1),\n",
        "        Dense(128, activation='selu'),\n",
        "        Dropout(0.1),\n",
        "        Dense(128, activation='selu'),\n",
        "        Dropout(0.1),\n",
        "        Dense(64, activation='selu'),\n",
        "        Dropout(0.1),\n",
        "        Dense(1)  # The output layer has one neuron to predict the lift-drag ratio\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Assuming the CST array has 14 parameters as input\n",
        "input_shape = 14  # Number of CST parameters\n",
        "\n",
        "# Build the DNN model\n",
        "dnn_model = build_dnn(input_shape)\n",
        "\n",
        "# Compile the model with the Adam optimizer and mean squared error loss function\n",
        "dnn_model.compile(optimizer=Adam(learning_rate=0.001),  # Start with a learning rate of 0.001\n",
        "                  loss='mean_squared_error')\n",
        "\n",
        "# Model Summary\n",
        "dnn_model.summary()\n",
        "\n",
        "# You would also need to set up the training procedure\n",
        "# Define the number of training iterations and the minibatch size\n",
        "training_iterations = 6000  # Training iterations\n",
        "batch_size = 64  # Minibatch size\n",
        "\n",
        "# The actual training code would involve fitting the model on your dataset\n",
        "# dnn_model.fit(x_train, y_train, epochs=training_iterations, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Ni-EAw0lob"
      },
      "source": [
        "DDQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rh-g9lAq0oMP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, optimizers\n",
        "import random\n",
        "import collections\n",
        "\n",
        "def get_initial_state(dataset):\n",
        "    index = np.random.choice(dataset.shape[0])\n",
        "    return dataset[index, :]\n",
        "\n",
        "# Assuming an environment function that returns the next state and reward given the current state and action\n",
        "def environment(state, action):\n",
        "    # This function needs to be defined based on your simulation environment\n",
        "    next_state = ...\n",
        "    reward = ...\n",
        "    return next_state, reward\n",
        "\n",
        "# The neural network architecture based on the given screenshot\n",
        "def create_q_network(num_states, num_actions, hidden_sizes):\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    x = inputs\n",
        "    for size in hidden_sizes:\n",
        "        x = layers.Dense(size, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_actions)(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Hyperparameters\n",
        "state_size = 14\n",
        "action_size = 28 # This is twice the number of elements in the state vector assuming two possible actions per element\n",
        "hidden_sizes = [128, 128, 128, 128]\n",
        "learning_rate = 0.001\n",
        "gamma = 0.95 # discount factor\n",
        "epsilon = 1.0 # exploration rate\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "tau = 0.15 # target network update rate\n",
        "\n",
        "# Create main and target networks\n",
        "main_network = create_q_network(state_size, action_size, hidden_sizes)\n",
        "target_network = create_q_network(state_size, action_size, hidden_sizes)\n",
        "target_network.set_weights(main_network.get_weights())\n",
        "\n",
        "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Replay buffer (to be defined based on your needs)\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size, state_size, action_size):\n",
        "        self.buffer = collections.deque(maxlen=max_size)\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        minibatch = random.sample(self.buffer, batch_size)\n",
        "        states = np.array([item[0] for item in minibatch])\n",
        "        actions = np.array([item[1] for item in minibatch])\n",
        "        rewards = np.array([item[2] for item in minibatch])\n",
        "        next_states = np.array([item[3] for item in minibatch])\n",
        "        dones = np.array([item[4] for item in minibatch])\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "buffer = ReplayBuffer(2000, state_size, action_size)\n",
        "\n",
        "# The training algorithm as per the pseudo code given\n",
        "for episode in range(1500):\n",
        "    state = np.random.rand(state_size)  # Replace with actual initial state\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(T):\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.rand() <= epsilon:\n",
        "            action = np.random.choice(action_size)\n",
        "        else:\n",
        "            action_values = main_network.predict(state.reshape(1, -1))\n",
        "            action = np.argmax(action_values[0])\n",
        "\n",
        "        next_state, reward = environment(state, action)\n",
        "        done = True if ... else False  # Condition to be replaced with actual terminal condition\n",
        "\n",
        "        buffer.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Sampling from the replay buffer\n",
        "        states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "\n",
        "        # Get Q values from target network\n",
        "        target_q_values = target_network.predict(next_states)\n",
        "        # Calculate the target values\n",
        "        targets = rewards + gamma * (np.amax(target_q_values, axis=1)) * (1 - dones)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = main_network(states)\n",
        "            # Select the Q value for the chosen action\n",
        "            q_action = tf.reduce_sum(tf.multiply(q_values, tf.one_hot(actions, action_size)), axis=1)\n",
        "            loss = tf.keras.losses.mean_squared_error(targets, q_action)\n",
        "\n",
        "        grads = tape.gradient(loss, main_network.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, main_network.trainable_variables))\n",
        "\n",
        "        # Update the target network\n",
        "        if t % C == 0:\n",
        "            new_weights = np.array(main_network.get_weights())\n",
        "            target_weights = np.array(target_network.get_weights())\n",
        "            target_network.set_weights(tau * new_weights + (1 - tau) * target_weights)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Decaying exploration rate\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    print(f\"Episode: {episode+1}, Total Reward: {total_reward}\")\n",
        "\n",
        "# Make sure to save your model\n",
        "main_network.save('ddqn_model.h5')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO9N8Kn2Lh4AuThyPSzkjbQ",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
